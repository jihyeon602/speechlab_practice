{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jihyeon602/speechlab_practice/blob/main/%5B%EC%8B%A4%EC%8A%B51%5D_DeepSpeech2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TqGPsmzKiUY"
      },
      "source": [
        "# DeepSpeech2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6iN54BqnSTpk"
      },
      "source": [
        "\n",
        "### Reference\n",
        "\n",
        "- Digital Signal Processing Lecture\n",
        "https://github.com/spatialaudio/digital-signal-processing-lecture\n",
        "\n",
        "- Python for Signal Processing (unipingco)\n",
        "https://github.com/unpingco/Python-for-Signal-Processing\n",
        "\n",
        "- Audio for Deep Learning (남기현님)\n",
        "https://tykimos.github.io/2019/07/04/ISS_2nd_Deep_Learning_Conference_All_Together/\n",
        "\n",
        "- 오디오 전처리 작업을 위한 연습 (박수철님)\n",
        "https://github.com/scpark20/audio-preprocessing-practice\n",
        "\n",
        "- Musical Applications of Machine Learning\n",
        "https://mac.kaist.ac.kr/~juhan/gct634/\n",
        "\n",
        "- Awesome audio study materials for Korean (최근우님)\n",
        "https://github.com/keunwoochoi/awesome-audio-study-materials-for-korean"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F4hfYNb2wu9m"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torchaudio\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bb06mH8jxGtr",
        "outputId": "e0f238ca-3703-4e83-b743-44ee9ec38240"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.95G/5.95G [09:06<00:00, 11.7MB/s]\n",
            "100%|██████████| 331M/331M [00:43<00:00, 7.90MB/s]\n"
          ]
        }
      ],
      "source": [
        "train_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"train-clean-100\", download=True)\n",
        "test_dataset = torchaudio.datasets.LIBRISPEECH(\"./\", url=\"test-clean\", download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8JvgqogxgtA",
        "outputId": "6896bc36-a9b7-4ef3-abf2-ba1bc8fe2157"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(tensor([[-0.0004, -0.0003, -0.0004,  ..., -0.0010, -0.0012, -0.0011]]),\n",
              " 16000,\n",
              " 'WELL NOW ENNIS I DECLARE YOU HAVE A HEAD AND SO HAS MY STICK',\n",
              " 1089,\n",
              " 134686,\n",
              " 10)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "test_dataset[10]\n",
        "#소리 데이터, 샘플레이트"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wlBQJ2QopRFk"
      },
      "outputs": [],
      "source": [
        "def avg_wer(wer_scores, combined_ref_len):\n",
        "    return float(sum(wer_scores)) / float(combined_ref_len)\n",
        "\n",
        "def _levenshtein_distance(ref, hyp):\n",
        "    \"\"\"\"Levenshtein distance\"는 두 시퀀스 간의 차이를 측정하기위한 문자열 메트릭입니다.\n",
        "    \"Levenshtein distanc\"는 한 단어를 다른 단어로 변경하는 데 필요한 최소 한 문자 편집 (대체, 삽입 또는 삭제) 수로 정의됩니다.\n",
        "    \"\"\"\n",
        "    m = len(ref)\n",
        "    n = len(hyp)\n",
        "\n",
        "    # special case\n",
        "    if ref == hyp:\n",
        "        return 0\n",
        "    if m == 0:\n",
        "        return n\n",
        "    if n == 0:\n",
        "        return m\n",
        "\n",
        "    if m < n:\n",
        "        ref, hyp = hyp, ref\n",
        "        m, n = n, m\n",
        "\n",
        "    # use O(min(m, n)) space\n",
        "    distance = np.zeros((2, n + 1), dtype=np.int32)\n",
        "\n",
        "    # initialize distance matrix\n",
        "    for j in range(0,n + 1):\n",
        "        distance[0][j] = j\n",
        "\n",
        "    # calculate levenshtein distance\n",
        "    for i in range(1, m + 1):\n",
        "        prev_row_idx = (i - 1) % 2\n",
        "        cur_row_idx = i % 2\n",
        "        distance[cur_row_idx][0] = i\n",
        "        for j in range(1, n + 1):\n",
        "            if ref[i - 1] == hyp[j - 1]:\n",
        "                distance[cur_row_idx][j] = distance[prev_row_idx][j - 1]\n",
        "            else:\n",
        "                s_num = distance[prev_row_idx][j - 1] + 1\n",
        "                i_num = distance[cur_row_idx][j - 1] + 1\n",
        "                d_num = distance[prev_row_idx][j] + 1\n",
        "                distance[cur_row_idx][j] = min(s_num, i_num, d_num)\n",
        "\n",
        "    return distance[m % 2][n]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ru55tshPqejt"
      },
      "outputs": [],
      "source": [
        "\n",
        "def word_errors(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"참조 시퀀스와 가설 시퀀스 사이의 거리를 단어 수준으로 계산합니다.\n",
        "     : param reference : 참조 문장.\n",
        "     : param hypothesis : 가설 문장.\n",
        "     : param ignore_case : 대소 문자 구분 여부.\n",
        "     : param delimiter : 입력 문장의 구분자.\n",
        "    \"\"\"\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    ref_words = reference.split(delimiter)\n",
        "    hyp_words = hypothesis.split(delimiter)\n",
        "\n",
        "    edit_distance = _levenshtein_distance(ref_words, hyp_words)\n",
        "    return float(edit_distance), len(ref_words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d-LCRpXVqhv8"
      },
      "outputs": [],
      "source": [
        "def char_errors(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    if ignore_case == True:\n",
        "        reference = reference.lower()\n",
        "        hypothesis = hypothesis.lower()\n",
        "\n",
        "    join_char = ' '\n",
        "    if remove_space == True:\n",
        "        join_char = ''\n",
        "\n",
        "    reference = join_char.join(filter(None, reference.split(' ')))\n",
        "    hypothesis = join_char.join(filter(None, hypothesis.split(' ')))\n",
        "\n",
        "    edit_distance = _levenshtein_distance(reference, hypothesis)\n",
        "    return float(edit_distance), len(reference)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gBZ_3-Sqm5D"
      },
      "outputs": [],
      "source": [
        "def wer(reference, hypothesis, ignore_case=False, delimiter=' '):\n",
        "    \"\"\"Calculate word error rate (WER).\n",
        "    WER = (Sw + Dw + Iw) / Nw\n",
        "    Sw는 대체 된 단어의 수입니다.\n",
        "    Dw는 삭제 된 단어의 수입니다.\n",
        "    Iw는 삽입 된 단어의 수입니다.\n",
        "    Nw는 참조의 단어 수입니다.\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = word_errors(reference, hypothesis, ignore_case,\n",
        "                                         delimiter)\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Reference's word number should be greater than 0.\")\n",
        "\n",
        "    wer = float(edit_distance) / ref_len\n",
        "    return wer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F6YUEk-2qqaw"
      },
      "outputs": [],
      "source": [
        "def cer(reference, hypothesis, ignore_case=False, remove_space=False):\n",
        "    \"\"\"Calculate charactor error rate (CER).\n",
        "        CER = (Sc + Dc + Ic) / Nc\n",
        "        Sc is the number of characters substituted,\n",
        "        Dc is the number of characters deleted,\n",
        "        Ic is the number of characters inserted\n",
        "        Nc is the number of characters in the reference\n",
        "    \"\"\"\n",
        "    edit_distance, ref_len = char_errors(reference, hypothesis, ignore_case,\n",
        "                                         remove_space)\n",
        "\n",
        "    if ref_len == 0:\n",
        "        raise ValueError(\"Length of reference should be greater than 0.\")\n",
        "\n",
        "    cer = float(edit_distance) / ref_len\n",
        "    return cer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOLv1UDe89em"
      },
      "source": [
        "The outputs of the network are the graphemes of each language. At each output time-step t, the RNN\n",
        "makes a prediction over characters, p(`t|x), where `t is either a character in the alphabet or the blank\n",
        "symbol. In English we have `t ∈ {a, b, c, . . . , z,space, apostrophe, blank},"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cS2jhIrBqs_G"
      },
      "outputs": [],
      "source": [
        "class TextTransform:\n",
        "    \"\"\"Maps characters to integers and vice versa\"\"\"\n",
        "    def __init__(self):\n",
        "        char_map_str = \"\"\"\n",
        "        ' 0\n",
        "        <SPACE> 1\n",
        "        a 2\n",
        "        b 3\n",
        "        c 4\n",
        "        d 5\n",
        "        e 6\n",
        "        f 7\n",
        "        g 8\n",
        "        h 9\n",
        "        i 10\n",
        "        j 11\n",
        "        k 12\n",
        "        l 13\n",
        "        m 14\n",
        "        n 15\n",
        "        o 16\n",
        "        p 17\n",
        "        q 18\n",
        "        r 19\n",
        "        s 20\n",
        "        t 21\n",
        "        u 22\n",
        "        v 23\n",
        "        w 24\n",
        "        x 25\n",
        "        y 26\n",
        "        z 27\n",
        "        \"\"\"\n",
        "        self.char_map = {}\n",
        "        self.index_map = {}\n",
        "        for line in char_map_str.strip().split('\\n'):\n",
        "            ch, index = line.split()\n",
        "            self.char_map[ch] = int(index)\n",
        "            self.index_map[int(index)] = ch\n",
        "        self.index_map[1] = ' '\n",
        "\n",
        "    def text_to_int(self, text):\n",
        "        \"\"\" Use a character map and convert text to an integer sequence \"\"\"\n",
        "        int_sequence = []\n",
        "        for c in text:\n",
        "            if c == ' ':\n",
        "                ch = self.char_map['<SPACE>']\n",
        "            else:\n",
        "                ch = self.char_map[c]\n",
        "            int_sequence.append(ch)\n",
        "        return int_sequence\n",
        "\n",
        "    def int_to_text(self, labels):\n",
        "        \"\"\" Use a character map and convert integer labels to an text sequence \"\"\"\n",
        "        string = []\n",
        "        for i in labels:\n",
        "            string.append(self.index_map[i])\n",
        "        return ''.join(string).replace('<SPACE>', ' ')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMUWPlL8qwQg",
        "outputId": "ef8a8397-bc00-48cf-8ee8-51aeced9b58a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchaudio/functional/functional.py:584: UserWarning: At least one mel filterbank has all zero values. The value for `n_mels` (128) may be set too high. Or, the value for `n_freqs` (201) may be set too low.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "train_audio_transforms = nn.Sequential(\n",
        "    torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=128),\n",
        "    torchaudio.transforms.FrequencyMasking(freq_mask_param=30),\n",
        "    torchaudio.transforms.TimeMasking(time_mask_param=100)\n",
        ")\n",
        "\n",
        "valid_audio_transforms = torchaudio.transforms.MelSpectrogram()\n",
        "\n",
        "text_transform = TextTransform()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VrbecqsFq8qM"
      },
      "outputs": [],
      "source": [
        "def data_processing(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, utterance, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        elif data_type == 'valid':\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            raise Exception('data_type should be train or valid')\n",
        "        spectrograms.append(spec)\n",
        "        label = torch.Tensor(text_transform.text_to_int(utterance.lower()))\n",
        "        labels.append(label)\n",
        "        input_lengths.append(spec.shape[0]//2)\n",
        "        label_lengths.append(len(label))\n",
        "\n",
        "    spectrograms = nn.utils.rnn.pad_sequence(spectrograms, batch_first=True).unsqueeze(1).transpose(2, 3)\n",
        "    labels = nn.utils.rnn.pad_sequence(labels, batch_first=True)\n",
        "\n",
        "    return spectrograms, labels, input_lengths, label_lengths\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HueYHhOMEiM"
      },
      "source": [
        "In both cases we integrate a language model in a beam search decoding step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZVpzOyHq9hX"
      },
      "outputs": [],
      "source": [
        "def GreedyDecoder(output, labels, label_lengths, blank_label=28, collapse_repeated=True):\n",
        "\targ_maxes = torch.argmax(output, dim=2)\n",
        "\tdecodes = []\n",
        "\ttargets = []\n",
        "\tfor i, args in enumerate(arg_maxes):\n",
        "\t\tdecode = []\n",
        "\t\ttargets.append(text_transform.int_to_text(labels[i][:label_lengths[i]].tolist()))\n",
        "\t\tfor j, index in enumerate(args):\n",
        "\t\t\tif index != blank_label:\n",
        "\t\t\t\tif collapse_repeated and j != 0 and index == args[j -1]:\n",
        "\t\t\t\t\tcontinue\n",
        "\t\t\t\tdecode.append(index.item())\n",
        "\t\tdecodes.append(text_transform.int_to_text(decode))\n",
        "\treturn decodes, targets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lQnVyQKU9PsY"
      },
      "source": [
        "We use the clipped rectifiedlinear (ReLU) function σ(x) = min{max{x, 0}, 20} as our nonlinearity.\n",
        "\n",
        "The architectures we experiment\n",
        "with consist of one or more convolutional layers, followed by one or more recurrent layers, followed\n",
        "by one or more fully connected layers.\n",
        "The hidden representation at layer l is given by h\n",
        "l with the convention that h\n",
        "0\n",
        "represents the input\n",
        "x. The bottom of the network is one or more convolutions over the time dimension of the input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XXAYnQ4arD2P"
      },
      "outputs": [],
      "source": [
        "class CNNLayerNorm(nn.Module):\n",
        "    \"\"\"Layer normalization built for cnns input\"\"\"\n",
        "    def __init__(self, n_feats):\n",
        "        super(CNNLayerNorm, self).__init__()\n",
        "        self.layer_norm = nn.LayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x (batch, channel, feature, time)\n",
        "        x = x.transpose(2, 3).contiguous() # (batch, channel, time, feature)\n",
        "        x = self.layer_norm(x)\n",
        "        return x.transpose(2, 3).contiguous() # (batch, channel, feature, time)\n",
        "\n",
        "class ResidualCNN(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel, stride, dropout, n_feats):\n",
        "        super(ResidualCNN, self).__init__()\n",
        "\n",
        "        self.cnn1 = nn.Conv2d(in_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.cnn2 = nn.Conv2d(out_channels, out_channels, kernel, stride, padding=kernel//2)\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "        self.layer_norm1 = CNNLayerNorm(n_feats)\n",
        "        self.layer_norm2 = CNNLayerNorm(n_feats)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x  # (batch, channel, feature, time)\n",
        "        x = self.layer_norm1(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout1(x)\n",
        "        x = self.cnn1(x)\n",
        "        x = self.layer_norm2(x)\n",
        "        x = F.gelu(x)\n",
        "        x = self.dropout2(x)\n",
        "        x = self.cnn2(x)\n",
        "        x += residual\n",
        "        return x # (batch, channel, feature, time)\n",
        "\n",
        "# The two sets of activations are summed to form the output activations for the layer h The function g(·) can be the standard recurrent operation\n",
        "class BidirectionalGRU(nn.Module):\n",
        "    def __init__(self, rnn_dim, hidden_size, dropout, batch_first):\n",
        "        super(BidirectionalGRU, self).__init__()\n",
        "\n",
        "        self.BiGRU = nn.GRU(\n",
        "            input_size=rnn_dim, hidden_size=hidden_size,\n",
        "            num_layers=1, batch_first=batch_first, bidirectional=True)\n",
        "        self.layer_norm = nn.LayerNorm(rnn_dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = F.gelu(x)\n",
        "        x, _ = self.BiGRU(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "class SpeechRecognitionModel(nn.Module):\n",
        "    def __init__(self, n_cnn_layers, n_rnn_layers, rnn_dim, n_class, n_feats, stride=2, dropout=0.1):\n",
        "        super(SpeechRecognitionModel, self).__init__()\n",
        "        n_feats = n_feats//2\n",
        "        self.cnn = nn.Conv2d(1, 32, 3, stride=stride, padding=3//2)  # cnn for extracting heirachal features\n",
        "\n",
        "        # n residual cnn layers with filter size of 32\n",
        "        self.rescnn_layers = nn.Sequential(*[\n",
        "            ResidualCNN(32, 32, kernel=3, stride=1, dropout=dropout, n_feats=n_feats)\n",
        "            for _ in range(n_cnn_layers)\n",
        "        ])\n",
        "        self.fully_connected = nn.Linear(n_feats*32, rnn_dim)\n",
        "        self.birnn_layers = nn.Sequential(*[\n",
        "            BidirectionalGRU(rnn_dim=rnn_dim if i==0 else rnn_dim*2,\n",
        "                             hidden_size=rnn_dim, dropout=dropout, batch_first=i==0)\n",
        "            for i in range(n_rnn_layers)\n",
        "        ])\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(rnn_dim*2, rnn_dim),  # birnn returns rnn_dim*2\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(rnn_dim, n_class)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cnn(x)\n",
        "        x = self.rescnn_layers(x)\n",
        "        sizes = x.size()\n",
        "        x = x.view(sizes[0], sizes[1] * sizes[2], sizes[3])  # (batch, feature, time)\n",
        "        x = x.transpose(1, 2) # (batch, time, feature)\n",
        "        x = self.fully_connected(x)\n",
        "        x = self.birnn_layers(x)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LMITaH3ALmr8"
      },
      "outputs": [],
      "source": [
        "\n",
        "import os\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class Dataset(Dataset):\n",
        "    def __init__(self, x):\n",
        "        self.x = x\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.x[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.x.shape[0]\n",
        "\n",
        "def load_dataset(data, data_type=\"train\"):\n",
        "    spectrograms = []\n",
        "    labels = []\n",
        "    input_lengths = []\n",
        "    label_lengths = []\n",
        "    for (waveform, _, _, _, _, _) in data:\n",
        "        if data_type == 'train':\n",
        "            spec = train_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        else:\n",
        "            spec = valid_audio_transforms(waveform).squeeze(0).transpose(0, 1)\n",
        "        spectrograms.append(spec)\n",
        "\n",
        "    return spectrograms\n",
        "\n",
        "def get_dataloader(data):\n",
        "    x_train = load_dataset(data, \"train\")\n",
        "    x_valid = load_dataset(data, 'valid')\n",
        "\n",
        "    mean = np.mean(x_train)\n",
        "    std = np.std(x_train)\n",
        "    x_train = (x_train - mean)/std\n",
        "    x_valid = (x_valid - mean)/std\n",
        "\n",
        "    train_set = Dataset(x_train)\n",
        "    vaild_set = Dataset(x_valid)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=4, shuffle=True, drop_last=False)\n",
        "    valid_loader = DataLoader(vaild_set, batch_size=4, shuffle=False, drop_last=False)\n",
        "\n",
        "    return train_loader, valid_loader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSRFH83jetB_"
      },
      "source": [
        "이 튜토리얼에서는 \"greedy\"디코딩 방법을 사용하여 모델의 출력을 문자로 결합하여 대화 내용을 만들 수 있습니다. \"greedy\"디코더는 모델의 소프트 맥스 확률 문자 인 모델 출력을 취하며 각 시간 단계에 대해 가장 높은 확률을 가진 레이블을 선택합니다. 라벨이 빈 라벨 인 경우 최종 성적표에서 라벨이 제거됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Omm1e-e4Mu0V"
      },
      "outputs": [],
      "source": [
        "class IterMeter(object):\n",
        "    \"\"\"keeps track of total iterations\"\"\"\n",
        "    def __init__(self):\n",
        "        self.val = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.val += 1\n",
        "\n",
        "    def get(self):\n",
        "        return self.val\n",
        "\n",
        "\n",
        "def train(model, device, train_loader, criterion, optimizer, epoch, iter_meter):\n",
        "    model.train()\n",
        "    data_len = len(train_loader.dataset)\n",
        "    for batch_idx, _data in enumerate(train_loader):\n",
        "        spectrograms, labels, input_lengths, label_lengths = _data\n",
        "        spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output = model(spectrograms)  # (batch, time, n_class)\n",
        "        output = F.log_softmax(output, dim=2)\n",
        "        output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "        loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        iter_meter.step()\n",
        "        if batch_idx % 100 == 0 or batch_idx == data_len:\n",
        "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(spectrograms), data_len,\n",
        "                100. * batch_idx / len(train_loader), loss.item()))\n",
        "\n",
        "\n",
        "def test(model, device, test_loader, criterion, epoch, iter_meter):\n",
        "    print('\\nevaluating...')\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    test_cer, test_wer = [], []\n",
        "    with torch.no_grad():\n",
        "        for i, _data in enumerate(test_loader):\n",
        "            spectrograms, labels, input_lengths, label_lengths = _data\n",
        "            spectrograms, labels = spectrograms.to(device), labels.to(device)\n",
        "\n",
        "            output = model(spectrograms)  # (batch, time, n_class)\n",
        "            # The output layer L is a softmax computing a probability distribution over characters given\n",
        "            output = F.log_softmax(output, dim=2)\n",
        "            output = output.transpose(0, 1) # (time, batch, n_class)\n",
        "\n",
        "            loss = criterion(output, labels, input_lengths, label_lengths)\n",
        "            test_loss += loss.item() / len(test_loader)\n",
        "\n",
        "            decoded_preds, decoded_targets = GreedyDecoder(output.transpose(0, 1), labels, label_lengths)\n",
        "            for j in range(len(decoded_preds)):\n",
        "                test_cer.append(cer(decoded_targets[j], decoded_preds[j]))\n",
        "                test_wer.append(wer(decoded_targets[j], decoded_preds[j]))\n",
        "\n",
        "\n",
        "    avg_cer = sum(test_cer)/len(test_cer)\n",
        "    avg_wer = sum(test_wer)/len(test_wer)\n",
        "\n",
        "    print('Test set: Average loss: {:.4f}, Average CER: {:4f} Average WER: {:.4f}\\n'.format(test_loss, avg_cer, avg_wer))\n",
        "\n",
        "\n",
        "def main(learning_rate=5e-4, batch_size=20, epochs=10,\n",
        "        train_url=\"train-clean-100\", test_url=\"test-clean\"):\n",
        "\n",
        "    hparams = {\n",
        "        \"n_cnn_layers\": 3,\n",
        "        \"n_rnn_layers\": 5,\n",
        "        \"rnn_dim\": 512,\n",
        "        \"n_class\": 29,\n",
        "        \"n_feats\": 128,\n",
        "        \"stride\":2,\n",
        "        \"dropout\": 0.1,\n",
        "        \"learning_rate\": learning_rate,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"epochs\": epochs\n",
        "    }\n",
        "\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    torch.manual_seed(7)\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if not os.path.isdir(\"./data\"):\n",
        "        os.makedirs(\"./data\")\n",
        "\n",
        "    train_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=train_url, download=True)\n",
        "    test_dataset = torchaudio.datasets.LIBRISPEECH(\"./data\", url=test_url, download=True)\n",
        "\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
        "    train_loader = data.DataLoader(dataset=train_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=True,\n",
        "                                collate_fn=lambda x: data_processing(x, 'train'),\n",
        "                                **kwargs)\n",
        "    test_loader = data.DataLoader(dataset=test_dataset,\n",
        "                                batch_size=hparams['batch_size'],\n",
        "                                shuffle=False,\n",
        "                                collate_fn=lambda x: data_processing(x, 'valid'),\n",
        "                                **kwargs)\n",
        "\n",
        "    model = SpeechRecognitionModel(\n",
        "        hparams['n_cnn_layers'], hparams['n_rnn_layers'], hparams['rnn_dim'],\n",
        "        hparams['n_class'], hparams['n_feats'], hparams['stride'], hparams['dropout']\n",
        "        ).to(device)\n",
        "\n",
        "    print('Num Model Parameters', sum([param.nelement() for param in model.parameters()]))\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), hparams['learning_rate'])\n",
        "    criterion = nn.CTCLoss(blank=28).to(device)\n",
        "\n",
        "    iter_meter = IterMeter()\n",
        "    for epoch in range(1, epochs + 1):\n",
        "        train(model, device, train_loader, criterion, optimizer, epoch, iter_meter)\n",
        "        test(model, device, test_loader, criterion, epoch, iter_meter)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HxNEtE7yP-DA",
        "outputId": "3085f652-a8b7-4c35-cd2f-f13314598752"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5.95G/5.95G [08:14<00:00, 12.9MB/s]\n",
            "100%|██████████| 331M/331M [00:27<00:00, 12.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Model Parameters 23705373\n",
            "Train Epoch: 1 [0/28539 (0%)]\tLoss: 6.524758\n",
            "Train Epoch: 1 [1000/28539 (4%)]\tLoss: 2.887502\n",
            "Train Epoch: 1 [2000/28539 (7%)]\tLoss: 2.880325\n",
            "Train Epoch: 1 [3000/28539 (11%)]\tLoss: 2.847404\n",
            "Train Epoch: 1 [4000/28539 (14%)]\tLoss: 2.871472\n",
            "Train Epoch: 1 [5000/28539 (18%)]\tLoss: 2.890348\n",
            "Train Epoch: 1 [6000/28539 (21%)]\tLoss: 2.862078\n",
            "Train Epoch: 1 [7000/28539 (25%)]\tLoss: 2.846669\n",
            "Train Epoch: 1 [8000/28539 (28%)]\tLoss: 2.951475\n",
            "Train Epoch: 1 [9000/28539 (32%)]\tLoss: 2.877874\n",
            "Train Epoch: 1 [10000/28539 (35%)]\tLoss: 2.898405\n",
            "Train Epoch: 1 [11000/28539 (39%)]\tLoss: 2.903080\n",
            "Train Epoch: 1 [12000/28539 (42%)]\tLoss: 2.872932\n",
            "Train Epoch: 1 [13000/28539 (46%)]\tLoss: 2.839333\n",
            "Train Epoch: 1 [14000/28539 (49%)]\tLoss: 2.847569\n",
            "Train Epoch: 1 [15000/28539 (53%)]\tLoss: 2.843788\n",
            "Train Epoch: 1 [16000/28539 (56%)]\tLoss: 2.846078\n",
            "Train Epoch: 1 [17000/28539 (60%)]\tLoss: 2.876750\n",
            "Train Epoch: 1 [18000/28539 (63%)]\tLoss: 2.880990\n",
            "Train Epoch: 1 [19000/28539 (67%)]\tLoss: 2.887585\n",
            "Train Epoch: 1 [20000/28539 (70%)]\tLoss: 2.918402\n",
            "Train Epoch: 1 [21000/28539 (74%)]\tLoss: 2.889632\n",
            "Train Epoch: 1 [22000/28539 (77%)]\tLoss: 2.894066\n",
            "Train Epoch: 1 [23000/28539 (81%)]\tLoss: 2.861099\n",
            "Train Epoch: 1 [24000/28539 (84%)]\tLoss: 2.879229\n",
            "Train Epoch: 1 [25000/28539 (88%)]\tLoss: 2.856899\n",
            "Train Epoch: 1 [26000/28539 (91%)]\tLoss: 2.809175\n",
            "Train Epoch: 1 [27000/28539 (95%)]\tLoss: 2.815710\n",
            "Train Epoch: 1 [28000/28539 (98%)]\tLoss: 2.848526\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 2.8625, Average CER: 0.941520 Average WER: 0.9997\n",
            "\n",
            "Train Epoch: 2 [0/28539 (0%)]\tLoss: 2.857719\n",
            "Train Epoch: 2 [1000/28539 (4%)]\tLoss: 2.842283\n",
            "Train Epoch: 2 [2000/28539 (7%)]\tLoss: 2.782727\n",
            "Train Epoch: 2 [3000/28539 (11%)]\tLoss: 2.807762\n",
            "Train Epoch: 2 [4000/28539 (14%)]\tLoss: 2.785209\n",
            "Train Epoch: 2 [5000/28539 (18%)]\tLoss: 2.769803\n",
            "Train Epoch: 2 [6000/28539 (21%)]\tLoss: 2.704409\n",
            "Train Epoch: 2 [7000/28539 (25%)]\tLoss: 2.635190\n",
            "Train Epoch: 2 [8000/28539 (28%)]\tLoss: 2.639282\n",
            "Train Epoch: 2 [9000/28539 (32%)]\tLoss: 2.575007\n",
            "Train Epoch: 2 [10000/28539 (35%)]\tLoss: 2.581263\n",
            "Train Epoch: 2 [11000/28539 (39%)]\tLoss: 2.557695\n",
            "Train Epoch: 2 [12000/28539 (42%)]\tLoss: 2.522465\n",
            "Train Epoch: 2 [13000/28539 (46%)]\tLoss: 2.527394\n",
            "Train Epoch: 2 [14000/28539 (49%)]\tLoss: 2.412416\n",
            "Train Epoch: 2 [15000/28539 (53%)]\tLoss: 2.401057\n",
            "Train Epoch: 2 [16000/28539 (56%)]\tLoss: 2.390374\n",
            "Train Epoch: 2 [17000/28539 (60%)]\tLoss: 2.407033\n",
            "Train Epoch: 2 [18000/28539 (63%)]\tLoss: 2.301175\n",
            "Train Epoch: 2 [19000/28539 (67%)]\tLoss: 2.300548\n",
            "Train Epoch: 2 [20000/28539 (70%)]\tLoss: 2.246617\n",
            "Train Epoch: 2 [21000/28539 (74%)]\tLoss: 2.276781\n",
            "Train Epoch: 2 [22000/28539 (77%)]\tLoss: 2.122959\n",
            "Train Epoch: 2 [23000/28539 (81%)]\tLoss: 2.144062\n",
            "Train Epoch: 2 [24000/28539 (84%)]\tLoss: 2.114605\n",
            "Train Epoch: 2 [25000/28539 (88%)]\tLoss: 1.988808\n",
            "Train Epoch: 2 [26000/28539 (91%)]\tLoss: 1.953461\n",
            "Train Epoch: 2 [27000/28539 (95%)]\tLoss: 2.073081\n",
            "Train Epoch: 2 [28000/28539 (98%)]\tLoss: 2.109384\n",
            "\n",
            "evaluating...\n",
            "Test set: Average loss: 1.8904, Average CER: 0.544283 Average WER: 1.1169\n",
            "\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 5e-4\n",
        "batch_size = 10\n",
        "epochs = 1\n",
        "libri_train_set = \"train-clean-100\"\n",
        "libri_test_set = \"test-clean\"\n",
        "\n",
        "main(learning_rate, batch_size, epochs, libri_train_set, libri_test_set)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}